{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkdaYL75ARWIshkyz1sT6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuraglahon16/ResearchPaperSimplify/blob/main/FinalProjectAIMcode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKsyEikiD9cA",
        "outputId": "00cad63f-f947-469b-9ddb-fd50f96a1736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.6/388.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m947.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.13.0\n",
            "OpenAI API Key:··········\n",
            "Enter the topic or keywords to search for papers: Lora fine tuning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The purpose of this conversation is to gather all the necessary details from you in order to help you understand the scientific concept of Lora fine tuning.\n",
            "\n",
            "What specific questions do you have about this topic?\n",
            "User: How to fine tune using Lora\n",
            "What is your current level of understanding of this topic?\n",
            "User: novice\n",
            "Are there any related concepts you'd like me to explain as well?\n",
            "User: Compare Lora and QLora\n",
            "Thank you for your responses. To summarize, you are looking to understand how to fine-tune using Lora, you are a novice in this topic, and you would like to compare Lora and QLora. I will provide you with explanations that will help you understand Lora fine tuning. <END-OF-CONVERSATION>\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI need to gather information on fine-tuning using Lora and compare it with QLora.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"fine-tuning using Lora\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mFine-Tuning, LoRA and QLoRA. In the realm of language models, fine tuning an existing language model to perform a specific task on specific data is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of the neural network through backpropagation during the training process. It is important to ... Fine-Tuning: Fine-tuning a model refers to the process of taking a pre-trained model (model trained on some big, public corpus) and further training it on a new, smaller dataset or with a specific ... November 24th, 2023. Low-Rank Adaptation (LoRA) method is a fine-tuning method introduced by a team of Microsoft researchers in 2021. Since then, it has become a very popular approach to fine-tuning large language models, diffusion models (such as for image-generation), and other types of AI models. LoRA is a type of Parameter-efficient Fine ... Fine tuning using the LoRA. Low-Rank Adaptation aka LoRA is a technique used to finetuning LLMs in a parameter efficient way. This doesn't involve finetuning whole of the base model, which can be huge and cost a lot of time and money. LoRA, instead adds a small number of trainable parameters to the model while keeping the original model ... LoRA is a technique that allows us to fine-tune large models like Stable Diffusion without the need to retrain them entirely, saving both time and computational resources. LoRA stands for Low-Rank Adaptation, a method designed to fine-tune large-scale models in a more efficient manner. The key idea behind LoRA is to update only a small part of ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to compare Lora and QLora to further understand the differences.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"Lora vs QLora comparison\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThis is where QLoRA and LoRA come into play. Brief introduction to gradient descent. On a very abstract level, An LLM is essentially a function that takes some input, processes it and outputs something. We can represent it as f(x, W) = y, where x is the input sequence, y is the output sequence, and W is the set of weights of the model that are ... QLORA extends LORA to enhance efficiency by quantizing weight values of the original network, from high-resolution data types, such as Float32, to lower resolution data types like int4. This leads ... Fine-Tuning Large Language Models Using PEFT. Let's put these concepts into practice with a code example of fine-tuning a large language model using QLORA. # Step 1: Load the pre-trained model and tokenizer. from transformers import BertTokenizer, BertForMaskedLM, QLORAdapter. model_name = \"bert-base-uncased\". QLoRA [2] is a quantized adaptation of LoRA [1] for fine-tuning large language models. Fine-tuning of very large models are excessively expensive, e.g., 16-bit fine-tuning of a LLaMA ( 65B ... Nonetheless, LoRA/ QLoRA continues to be a highly effective method for parameter efficient fine-tuning and is widely used. Conclusion. Low Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right configuration. Choosing the correct value of rank and the layers of the neural network architecture to ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: LoRA is a technique used to fine-tune large models in a parameter-efficient way, while QLoRA extends LoRA by quantizing weight values to enhance efficiency further.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer:\n",
            "LoRA is a technique used to fine-tune large models in a parameter-efficient way, while QLoRA extends LoRA by quantizing weight values to enhance efficiency further.\n",
            "\n",
            "Recommended Papers:\n",
            "content='1. \"A Survey of Machine Learning Techniques for Social Media Mining\" by Kamalika Das, Amitava Das, and Partha Pratim Ray\\n2. \"Sentiment Analysis in Social Media\" by Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau\\n3. \"Mining Social Media Data for Understanding Students\\' Learning Experiences\" by Yi-Shin Chen, Hsin-Yi Tang, and Shian-Shyong Tseng' response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 56, 'total_tokens': 163}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U\n",
        "!pip install arxiv pymupdf faiss-cpu -q -U\n",
        "!pip install chromadb -q\n",
        "!pip install tenacity -q\n",
        "!pip install duckduckgo-search -q\n",
        "!pip install scholarly -q\n",
        "!pip install xmltodict\n",
        "\n",
        "# Set up environment variables\n",
        "import os\n",
        "import getpass\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key:\")\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA, ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.utilities import DuckDuckGoSearchAPIWrapper, PubMedAPIWrapper, ArxivAPIWrapper\n",
        "\n",
        "# Get user input for the paper to search\n",
        "search_query = input(\"Enter the topic or keywords to search for papers: \")\n",
        "\n",
        "# Load scientific papers from arXiv\n",
        "loader = ArxivLoader(query=search_query, load_max_docs=10)\n",
        "files = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = CharacterTextSplitter(separator='\\n---\\n', chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(files)\n",
        "\n",
        "# Create vector database and store papers\n",
        "embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-large\")\n",
        "db = Chroma.from_documents(docs, embedding_model, persist_directory='./chroma_db')\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=150)\n",
        "\n",
        "# Initialize the conversation memory\n",
        "chat_memory = ConversationBufferMemory(ai_prefix=\"AI Assistant\")\n",
        "\n",
        "# Define the conversation template\n",
        "template = \"\"\"\n",
        "You're an AI assistant and your task is to gather all details from a user who wants to understand a scientific concept.\n",
        "\n",
        "At the beginning, shortly describe the purpose of this conversation.\n",
        "\n",
        "You should gather answers for the following questions:\n",
        "\n",
        "- What specific questions do you have about this topic?\n",
        "- What is your current level of understanding of this topic?\n",
        "- Are there any related concepts you'd like me to explain as well?\n",
        "\n",
        "Don't answer the question you are asking.\n",
        "\n",
        "Be patient and encouraging if the user doesn't know how to answer some questions, and help guide them.\n",
        "\n",
        "Ask one question at a time.\n",
        "\n",
        "Once you have gathered all the details, thank the user for their responses, summarize the relevant information that will help you provide the best explanation, and put '<END-OF-CONVERSATION>'\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI assistant:\n",
        "\"\"\"\n",
        "\n",
        "# Create the conversation prompt\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "conversation = ConversationChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=chat_memory\n",
        ")\n",
        "\n",
        "# Start the conversation\n",
        "current_input = f\"I want to learn about {search_query}\"\n",
        "\n",
        "end_seq = '<END-OF-CONVERSATION>'\n",
        "user_requirements = ''\n",
        "\n",
        "while True:\n",
        "    ai_response = conversation.predict(input=current_input)\n",
        "    print(ai_response)\n",
        "\n",
        "    if end_seq in ai_response:\n",
        "        user_requirements = chat_memory.chat_memory.messages[-1].content.replace(end_seq, '')\n",
        "        break\n",
        "\n",
        "    user_input = input('User: ')\n",
        "    current_input = user_input\n",
        "\n",
        "# Initialize the DuckDuckGo search tool\n",
        "search = DuckDuckGoSearchAPIWrapper()\n",
        "\n",
        "# Initialize the RAG tool\n",
        "rag_tool = Tool(\n",
        "    name=\"RAG System\",\n",
        "    func=lambda q: RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever()).run(q),\n",
        "    description=\"Useful for answering questions based on the retrieved papers.\"\n",
        ")\n",
        "\n",
        "# Initialize the PubMed search tool\n",
        "pubmed_search = PubMedAPIWrapper()\n",
        "\n",
        "# Initialize the arXiv search tool\n",
        "arxiv_search = ArxivAPIWrapper()\n",
        "\n",
        "tool_belt = [\n",
        "    Tool(\n",
        "        name=\"DuckDuckGo Search\",\n",
        "        func=search.run,\n",
        "        description=\"Useful for searching for additional information on the internet.\"\n",
        "    ),\n",
        "    rag_tool,\n",
        "    Tool(\n",
        "        name=\"PubMed Search\",\n",
        "        func=pubmed_search.run,\n",
        "        description=\"Useful for searching for biomedical literature on PubMed.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"arXiv Search\",\n",
        "        func=arxiv_search.run,\n",
        "        description=\"Useful for searching for papers on arXiv.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Generate the answer using the agent with retry logic\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def generate_answer(query):\n",
        "    agent = initialize_agent(tool_belt, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "    return agent.run(f\"Use the available tools to answer the query: {query}. If one tool doesn't provide a satisfactory answer, try using the other tools.\")\n",
        "\n",
        "try:\n",
        "    answer = generate_answer(user_requirements)\n",
        "except Exception as e:\n",
        "    print(f\"Error generating answer: {str(e)}\")\n",
        "    answer = \"I apologize, I encountered an error while generating the answer. I tried using the available tools, but I still don't have enough relevant information to provide a satisfactory explanation. I would recommend searching for more specific papers or resources related to the topic to get a better understanding.\"\n",
        "\n",
        "# Retrieve similar papers\n",
        "similar_papers = db.similarity_search(user_requirements, k=3)\n",
        "\n",
        "if similar_papers:\n",
        "    # Generate a prompt for recommending similar papers\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"papers\"],\n",
        "        template=\"\"\"\n",
        "        Based on the user's requirements and the retrieved papers, recommend 3 similar papers that could help the user better understand the topic.\n",
        "\n",
        "        Retrieved Papers:\n",
        "        {papers}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Format the similar papers into a string\n",
        "    similar_papers_str = \"\\n\".join([f\"- {paper.metadata.get('title', 'Title not available')}\" for paper in similar_papers])\n",
        "\n",
        "    # Create the prompt for recommending similar papers\n",
        "    recommend_prompt = prompt_template.format(papers=similar_papers_str)\n",
        "\n",
        "    # Generate the recommendation using the language model\n",
        "    recommendation = llm.invoke(recommend_prompt)\n",
        "else:\n",
        "    recommendation = \"I apologize, I couldn't find any similar papers based on the user's requirements. The loaded papers do not seem to contain enough relevant information about the topic. I would suggest searching for more specific papers related to the topic to get a better understanding.\"\n",
        "\n",
        "# Print the answer and recommendation\n",
        "print(\"Answer:\")\n",
        "print(answer)\n",
        "print(\"\\nRecommended Papers:\")\n",
        "print(recommendation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T23yJQHoVcx_"
      }
    }
  ]
}